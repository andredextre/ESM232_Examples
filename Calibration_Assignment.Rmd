---
title: 'Choosing models - evaluating performance'
output:
  slidy_presentation:
    highlight: pygments
  html_document: default
  pdf_document: default
  ioslides_presentation:
    highlight: pygments
  beamer_presentation:
    highlight: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(sensitivity)
library(tidyverse)
library(lubridate)
library(reldist)
library(purrr)
library(ggpubr)
```

# Assignment

Final piece will be to produce a graph of maximum likelihood estimate given you acceptable parameters!

To hand in - an Rmarkdown and R function. Please knit and turn in either an html or pdf of the markdown. 

* Part 1 from above: R function that codes a metric for performance evaluation 
  * must be a combination of at least two performance measures
  * include some comments that explain 'why' this metric
  
```{r}
#Load Sager data
sager <- read.table("Data/sager.txt", header=T)

#Create function
stream_five_yr <- function(model, obs, wy, end_year) {
  
  #Get start year by subracting from input year (5-year)
  start_year <- end_year - 5
  #Assign parameters to a dataframe
  df <- as.data.frame(cbind(model, obs, wy)) %>% 
    filter(wy %in% c(start_year:end_year))
  
  #Calculate flows
  obs_flow <- sum(df$obs)
  mod_flow <- sum(df$mod)
  
  #Calculate error
  error <- 1 - abs(mod_flow - obs_flow)/obs_flow
  #Calculate Correlation
  correlation <- cor(df$model, df$obs)
  #Get combined metric
  combined <- 0.5*correlation + 0.5*error
  #Print results
  print(combined)
}

#Call Function
source("R/stream_function.R")

#Test function
stream_five_yr(sager$model, sager$obs, sager$wy, 1979)
```
  
  
* R markdown that does the following steps (with lots of documentation of the work flow):

  * Part 2 from above: 
    1. Apply your performance function to a subset of the Sagehen data set (with multiple simulations) that you want to use for calibration 
    2. Summarize the performance over the calibration period in 1-2 graphs; you can decide what is useful 
  
```{r}
# multiple results - lets say we've run the model for multiple years, 
#each column  is streamflow for a different parameter set
msage <- read.table("Data/sagerm.txt", header=T)

# keep track of number of simulations (e.g results for each parameter set) 
# use as a column names
nsim <- ncol(msage)
snames <- sprintf("S%d",seq(from=1, to=nsim))
colnames(msage)=snames


# lets say we know the start date from our earlier output
msage$date = sager$date
msage$month = sager$month
msage$year = sager$year
msage$day = sager$day
msage$wy = sager$wy

# lets add observed
msage <- left_join(msage, sager[,c("obs","date")], by = c("date"))

head(msage)

# how can we plot all results - lets plot water year 1970 otherwise its hard to see
msagel <- msage %>%
  pivot_longer(cols=!c(date,
                       month,
                       year,
                       day,
                       wy),
               names_to="run",
               values_to="flow")

p1 <- ggplot(subset(msagel,
                    wy == 1970),
             aes(as.Date(date),
                 flow, col=run)) + 
  geom_line() + 
  theme(legend.position = "none")

p1

# lets add observed stream-flow
p1 + geom_line(data = subset(sager, 
                             wy == 1970),
               aes(as.Date(date),
                   obs),
               size = 2,
               col = "black",
               linetype = 2) + 
  labs(y = "Streamflow",
       x = "Date")

# subset for split sample calibration
short_msage <- subset(msage, wy < 1975)

# compute performance measures for output from all parameters
res <- short_msage %>% select(!c("date","month","year","day","wy","obs")) %>%
      map_dbl(nse, short_msage$obs) # purrr function here! map_dbl will apply the function nse() to each column in our data frame against the observed and returns a vector

head(res)


# another example using our low flow statistics
# use apply to compute for all the data
source("../R/compute_lowflowmetrics_all.R")
res = short_msage %>% select(-date, -month, -day, -year, -wy, -obs ) %>%
  map_df(compute_lowflowmetrics_all, o=short_msage$obs, month=short_msage$month, day=short_msage$day, year=short_msage$year, wy=short_msage$wy)
# note here we use map_df to get a dataframe back 


# interesting to look at range of metrics - could use this to decide on
# acceptable values
summary(res)
# we can add a row that links with simulation number
res$sim = snames

# graph range of performance measures
resl = res %>% pivot_longer(-sim, names_to="metric", values_to="value")

ggplot(resl, aes(metric, value))+geom_boxplot()+facet_wrap(~metric, scales="free")


# select the best one based on the combined metric
best = res[which.max(res$combined),]

# running the model forward
# so we can look at the full time series

# lets start with streamflow estimates from best performing parameter set
 ggplot(msage, aes(date, msage[,best$sim])) + geom_line()+geom_line(aes(date, obs), col="red") 

 
# for comparison lets consider how worst and best parameters perform for subsequent simulations
# focusing specifically on August streamflow
 worst = res[which.min(res$combined),]
 
 compruns = msage %>% select(best$sim, worst$sim, date, obs, month, day, year, wy)
 compruns = subset(compruns, wy > 1970)
 compruns_mwy = compruns %>% select(-c(day,date, year)) %>% group_by(month, wy) %>% summarize(across(everything(), mean))
 
 compruns_mwyl = compruns_mwy %>% pivot_longer(cols=!c(month,wy), names_to="sim", values_to="flow")
 compruns_mwyl %>% subset(month==8) %>% ggplot(aes(sim,flow ))+geom_boxplot()
```


  * Part 3  
    3. Use the performance measure to select "acceptable" outcomes from parameter sets (see #15 in contents)
    4. Compute the range of the performance measure using only the "acceptable" outcomes over the post-calibration period (part that you didn't use for calibration in step 1)
    5. Graph the range of outcomes for acceptable parameters (e.g post-calibration parameter uncertainty); you can choose what output is most interesting for you 
    6. Compute and graph the maximum likelihood estimate of your output of interest (e.g minimum summer streamflow each year) for the post-calibration period (see #16 or #17 in contents)


```{r}

```


  * Part 4: A short paragraph discussing why you choose the output and performance measures that you did and some thoughts (1-2 sentences) on what your calibration and post-calibration uncertainty analysis tells you
  
```{r}

```


# Rubric 60 pts 

* R function (10pts) 
  * combines at least 2 performance metrics (5)
  * function is applied to part of Sagehen data set (5)
  
* Calibration (20pts)
  * your metrics are used to select 'acceptable' parameter set outcomes (5)
  * metrics are computed for post-calibration data of accepted parameter set outcomes (5)
  * maximum likelihood estimate is computed for post-calibration data (10)
  
* Graphs (20pts)
  * 1-2 plots of summary of performance over calibration period (5) 
  * 1-2 plots of output of acceptable parameter sets that clearly visualize uncertainty (5)
  * plot maximum likelihood estimate for post-calibration period (5) 
  * graphing style (axis labels, legibility) (5)
  
* Discussion (10pts)
  * short explanation on metrics used (5) 
  * 1-2 sentences on calibration and post-calibration uncertainty analysis 



